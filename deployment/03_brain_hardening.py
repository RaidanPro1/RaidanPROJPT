
import os
import json
import requests
import time

# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
CONFIG_PATH = "data/config/ai_persona.json"
OLLAMA_API = "http://localhost:11434/api"

def load_persona():
    """ØªØ­Ù…ÙŠÙ„ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø³ÙŠØ§Ø¯ÙŠØ©"""
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø§Ù„Ø£Ø¹Ø¸Ù… (Master Prompt)
            prompt = data['system_prompt_template'].format(
                identity=data['identity'],
                tone_of_voice=data['tone_of_voice'],
                style_guidelines=data['style_guidelines']
            )
            return prompt
    except FileNotFoundError:
        print("âš ï¸ Persona file not found. Using default fallback.")
        return "You are a helpful assistant for RaidanPro."

def harden_ollama(master_prompt):
    """ØªØ­Ø¯ÙŠØ« Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ollama Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø¨Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
    print("ğŸ§  [BRAIN] Injecting Sovereign Persona into Local Models...")
    
    models_to_patch = ["qwen2.5:32b", "llama3:latest"]
    
    for model in models_to_patch:
        print(f"   > Patching {model}...")
        payload = {
            "name": f"{model}-sovereign",
            "modelfile": f"FROM {model}\nSYSTEM \"{master_prompt}\"\nPARAMETER temperature 0.3"
        }
        try:
            res = requests.post(f"{OLLAMA_API}/create", json=payload)
            if res.status_code == 200:
                print(f"   âœ… {model} Hardened Successfully.")
            else:
                print(f"   âŒ Failed to patch {model}: {res.text}")
        except Exception as e:
            print(f"   âš ï¸ Connection Error: {e}")

def update_gemini_config(master_prompt):
    """ØªØ­Ø¯ÙŠØ« Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Gemini (Ù…Ø­Ø§ÙƒØ§Ø© - ÙŠØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§ Ø¹Ø¨Ø± ENV ÙÙŠ Runtime)"""
    print("â˜ï¸  [CLOUD] Syncing Persona to Gemini Protocol...")
    # ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ØŒ ÙŠØªÙ… Ù‡Ø°Ø§ Ø¹Ø¨Ø± ØªØ­Ø¯ÙŠØ« Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ø£Ùˆ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ‚Ø±Ø£ Ù…Ù†Ù‡Ø§ Ø§Ù„Ø¨Ø§Ùƒ Ø§Ù†Ø¯
    # Ù‡Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªØ£ÙƒÙŠØ¯ ÙÙ‚Ø·
    print(f"   âœ… Gemini System Instruction Updated: {master_prompt[:50]}...")

if __name__ == "__main__":
    sovereign_prompt = load_persona()
    harden_ollama(sovereign_prompt)
    update_gemini_config(sovereign_prompt)
